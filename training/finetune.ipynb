{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GeneticLLM Fine-tuning with QLoRA\n",
        "\n",
        "Fine-tune a small LLM on genetic research Q&A using QLoRA (4-bit quantization + LoRA).\n",
        "\n",
        "**Requirements:** Free Google Colab with T4 GPU\n",
        "\n",
        "**Base Model:** Qwen2-1.5B-Instruct (small, capable, Apache 2.0 license)\n",
        "\n",
        "**Dataset:** [sachinbkale27/genetics-qa](https://huggingface.co/datasets/sachinbkale27/genetics-qa) (89k samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q unsloth\n",
        "!pip install -q --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q datasets transformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Check GPU\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Base Model with 4-bit Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"unsloth/Qwen2-1.5B-Instruct-bnb-4bit\"\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "# Load model with Unsloth (2x faster, 50% less memory)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank (higher = more capacity, more memory)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Long context support\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Dataset from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset directly from HuggingFace Hub\n",
        "DATASET_NAME = \"sachinbkale27/genetics-qa\"\n",
        "\n",
        "print(f\"Loading dataset: {DATASET_NAME}\")\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "print(f\"\\nDataset loaded!\")\n",
        "print(f\"Training samples: {len(dataset['train'])}\")\n",
        "print(f\"Validation samples: {len(dataset['validation'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format for training\n",
        "def format_prompt(sample):\n",
        "    \"\"\"Convert messages format to training text.\"\"\"\n",
        "    messages = sample[\"messages\"]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Format the training split\n",
        "train_dataset = dataset[\"train\"].map(format_prompt)\n",
        "\n",
        "print(f\"Formatted {len(train_dataset)} training samples\")\n",
        "print(f\"\\nSample prompt:\\n{train_dataset[0]['text'][:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "# Note: With 89k samples, we use 1 epoch and adjust batch size\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=1,  # 1 epoch with 89k samples is sufficient\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(f\"Total training samples: {len(train_dataset)}\")\n",
        "print(f\"Batch size: 4 x 4 (gradient accumulation) = 16 effective\")\n",
        "print(f\"Estimated steps: {len(train_dataset) // 16}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n",
        "print(f\"Total steps: {trainer_stats.global_step}\")\n",
        "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Switch to inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def ask_genetics_question(question: str, context: str = \"\") -> str:\n",
        "    \"\"\"Query the fine-tuned model.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a genetic research assistant with expertise in molecular biology, genomics, and genetic analysis.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\" if context else question}\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract assistant response\n",
        "    return response.split(\"assistant\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test questions\n",
        "test_questions = [\n",
        "    \"What is the role of CRISPR-Cas9 in gene editing?\",\n",
        "    \"How do single nucleotide polymorphisms (SNPs) affect disease risk?\",\n",
        "    \"Explain the process of DNA methylation and its impact on gene expression.\",\n",
        "    \"What is the difference between genotype and phenotype?\",\n",
        "    \"How does mRNA translation work?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {ask_genetics_question(q)}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters only (small, ~50MB)\n",
        "model.save_pretrained(\"genetic-llm-lora\")\n",
        "tokenizer.save_pretrained(\"genetic-llm-lora\")\n",
        "\n",
        "print(\"LoRA adapters saved to genetic-llm-lora/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the LoRA adapters\n",
        "!zip -r genetic-llm-lora.zip genetic-llm-lora/\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"genetic-llm-lora.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Push to Hugging Face Hub (Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push your fine-tuned model to HuggingFace for easy sharing\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login with your token\n",
        "login(token=\"YOUR_HF_TOKEN\")  # Replace with your token\n",
        "\n",
        "# Push the model\n",
        "model.push_to_hub(\"YOUR_USERNAME/genetic-llm\", tokenizer=tokenizer)\n",
        "print(\"Model pushed to Hugging Face Hub!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
