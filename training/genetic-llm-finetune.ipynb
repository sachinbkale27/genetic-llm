{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GeneticLLM Fine-tuning with QLoRA\n",
        "\n",
        "Fine-tune Qwen2-1.5B on genetic research Q&A using QLoRA.\n",
        "\n",
        "**GPU:** T4 (Free Colab) or V100/A100\n",
        "\n",
        "**Dataset:** [sachinbkale27/genetics-qa](https://huggingface.co/datasets/sachinbkale27/genetics-qa) (89k samples)\n",
        "\n",
        "**Tracking:** Weights & Biases (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q unsloth wandb\n",
        "!pip install -q --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q datasets transformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Weights & Biases from Colab Secrets\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "    wandb.login(key=wandb_api_key)\n",
        "    USE_WANDB = True\n",
        "    print(\"âœ“ W&B login successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"W&B login failed: {e}\")\n",
        "    print(\"Training will continue without W&B logging.\")\n",
        "    print(\"To enable: Add WANDB_API_KEY to Colab Secrets (ðŸ”‘ icon in sidebar)\")\n",
        "    USE_WANDB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import time\n",
        "\n",
        "# Check GPU\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f\"GPU: {gpu_name}\")\n",
        "print(f\"Memory: {gpu_mem:.1f} GB\")\n",
        "\n",
        "# Auto-detect GPU and set batch sizes\n",
        "if \"A100\" in gpu_name:\n",
        "    print(\"âœ“ A100 detected\")\n",
        "    BATCH_SIZE = 16\n",
        "    GRAD_ACCUM = 4\n",
        "elif \"V100\" in gpu_name:\n",
        "    print(\"âœ“ V100 detected\")\n",
        "    BATCH_SIZE = 8\n",
        "    GRAD_ACCUM = 4\n",
        "else:\n",
        "    print(\"âœ“ T4/Other detected\")\n",
        "    BATCH_SIZE = 4\n",
        "    GRAD_ACCUM = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"unsloth/Qwen2-1.5B-Instruct-bnb-4bit\"\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "DATASET_NAME = \"sachinbkale27/genetics-qa\"\n",
        "\n",
        "print(f\"Loading dataset: {DATASET_NAME}\")\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "print(f\"\\nTraining samples: {len(dataset['train'])}\")\n",
        "print(f\"Validation samples: {len(dataset['validation'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format for training\n",
        "def format_prompt(sample):\n",
        "    messages = sample[\"messages\"]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "train_dataset = dataset[\"train\"].map(format_prompt, num_proc=2)\n",
        "print(f\"Formatted {len(train_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration with W&B\n",
        "effective_batch = BATCH_SIZE * GRAD_ACCUM\n",
        "total_steps = len(train_dataset) // effective_batch\n",
        "\n",
        "print(f\"Effective batch size: {effective_batch}\")\n",
        "print(f\"Total steps: {total_steps}\")\n",
        "print(f\"W&B logging: {'Enabled' if USE_WANDB else 'Disabled'}\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=1,\n",
        "        max_steps=5000,  # Safety limit for T4\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=1000,\n",
        "        report_to=\"wandb\" if USE_WANDB else \"none\",\n",
        "        run_name=\"genetic-llm\" if USE_WANDB else None,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting fine-tuning...\")\n",
        "start_time = time.time()\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "elapsed = (time.time() - start_time) / 60\n",
        "print(f\"\\nTraining complete!\")\n",
        "print(f\"Total time: {elapsed:.1f} minutes\")\n",
        "print(f\"Total steps: {trainer_stats.global_step}\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def ask(question):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a genetic research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=256, temperature=0.7, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"assistant\")[-1].strip()\n",
        "\n",
        "print(\"Q: What is CRISPR-Cas9?\")\n",
        "print(f\"A: {ask('What is CRISPR-Cas9?')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save and download model\n",
        "model.save_pretrained(\"genetic-llm-lora\")\n",
        "tokenizer.save_pretrained(\"genetic-llm-lora\")\n",
        "print(\"Model saved!\")\n",
        "\n",
        "!zip -r genetic-llm-lora.zip genetic-llm-lora/\n",
        "from google.colab import files\n",
        "files.download(\"genetic-llm-lora.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Push to HuggingFace\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"YOUR_HF_TOKEN\")\n",
        "# model.push_to_hub(\"sachinbkale27/genetic-llm\", tokenizer=tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
